{
  "name": "QwenSuiCoder",
  "description": "End-to-end LLM Benchmarking & Training Framework for Sui blockchain development",
  "tech_stack": [
    "Python",
    "DeepSpeed",
    "Qwen 2.5",
    "MLOps",
    "PyTorch"
  ],
  "github": "https://github.com/Angleito/qwensuicoder",
  "private_full_version": false,
  "contact": "arainey555@gmail.com",
  "features": [
    "Hardware-aware model selection",
    "Automated benchmarking (0.5B to 14B parameters)",
    "Smart parameter selection",
    "DeepSpeed-optimized training with ZeRO optimization",
    "Progressive quantization testing"
  ],
  "body": {
    "raw": "\n## QwenSuiCoder: Automated LLM Benchmarking & Training Framework\n\nThis project is an end-to-end solution for benchmarking, optimizing, and fine-tuning large language models (specifically Qwen 2.5 models) specialized for Sui blockchain development. It's designed to intelligently determine the optimal model size, quantization level, and context length that your hardware can efficiently handle.\n\n### Core Components\n\n1. **Automated Benchmarking** (`benchmark_models.py`):\n   - Tests various model sizes (0.5B to 14B parameters) with different quantization techniques (FP16, 8-bit, 4-bit)\n   - Measures VRAM usage, loading times, and inference capabilities\n   - Identifies the largest model your hardware can run efficiently\n   - Outputs detailed metrics in JSON format for further analysis\n\n2. **Smart Parameter Selection** (`run_benchmarks.sh`):\n   - Orchestrates the benchmarking process\n   - Analyzes results to determine optimal:\n     - Model size (e.g., Qwen 2.5 7B vs 14B)\n     - Quantization level (FP16, 8-bit, 4-bit)\n     - Context length (based on VRAM constraints)\n   - Generates a customized training script with optimized parameters\n\n3. **DeepSpeed-Optimized Training** (`optimized_training.py`):\n   - Implements efficient training using DeepSpeed's ZeRO optimization stages\n   - Supports gradient accumulation, mixed-precision training, and CPU offloading\n   - Configures memory-efficient training based on benchmark results\n   - Saves trained models with standardized naming conventions\n\n### Technical Innovations\n\n1. **Hardware-Aware Model Selection**: Automatically selects the largest model that fits within available VRAM, rather than requiring manual trial and error.\n\n2. **Progressive Quantization Testing**: Tests models with decreasing precision (FP16 → 8-bit → 4-bit) to find the optimal balance between model size and performance.\n\n3. **Robust Error Handling**: Incorporates fallback mechanisms to ensure the pipeline continues even if certain benchmarking steps fail.\n\n4. **Efficient Resource Utilization**: Configures DeepSpeed parameters (ZeRO stages, offloading, etc.) based on hardware constraints for maximum throughput.\n\n### Note\n\nThis project is now publicly available on GitHub. You can view the code repository at [https://github.com/Angleito/qwensuicoder](https://github.com/Angleito/qwensuicoder). For any questions or collaborations, feel free to contact me at arainey555@gmail.com. \n\n",
    "html": "<h2>QwenSuiCoder: Automated LLM Benchmarking &#x26; Training Framework</h2>\n<p>This project is an end-to-end solution for benchmarking, optimizing, and fine-tuning large language models (specifically Qwen 2.5 models) specialized for Sui blockchain development. It's designed to intelligently determine the optimal model size, quantization level, and context length that your hardware can efficiently handle.</p>\n<h3>Core Components</h3>\n<ol>\n<li>\n<p><strong>Automated Benchmarking</strong> (<code>benchmark_models.py</code>):</p>\n<ul>\n<li>Tests various model sizes (0.5B to 14B parameters) with different quantization techniques (FP16, 8-bit, 4-bit)</li>\n<li>Measures VRAM usage, loading times, and inference capabilities</li>\n<li>Identifies the largest model your hardware can run efficiently</li>\n<li>Outputs detailed metrics in JSON format for further analysis</li>\n</ul>\n</li>\n<li>\n<p><strong>Smart Parameter Selection</strong> (<code>run_benchmarks.sh</code>):</p>\n<ul>\n<li>Orchestrates the benchmarking process</li>\n<li>Analyzes results to determine optimal:\n<ul>\n<li>Model size (e.g., Qwen 2.5 7B vs 14B)</li>\n<li>Quantization level (FP16, 8-bit, 4-bit)</li>\n<li>Context length (based on VRAM constraints)</li>\n</ul>\n</li>\n<li>Generates a customized training script with optimized parameters</li>\n</ul>\n</li>\n<li>\n<p><strong>DeepSpeed-Optimized Training</strong> (<code>optimized_training.py</code>):</p>\n<ul>\n<li>Implements efficient training using DeepSpeed's ZeRO optimization stages</li>\n<li>Supports gradient accumulation, mixed-precision training, and CPU offloading</li>\n<li>Configures memory-efficient training based on benchmark results</li>\n<li>Saves trained models with standardized naming conventions</li>\n</ul>\n</li>\n</ol>\n<h3>Technical Innovations</h3>\n<ol>\n<li>\n<p><strong>Hardware-Aware Model Selection</strong>: Automatically selects the largest model that fits within available VRAM, rather than requiring manual trial and error.</p>\n</li>\n<li>\n<p><strong>Progressive Quantization Testing</strong>: Tests models with decreasing precision (FP16 → 8-bit → 4-bit) to find the optimal balance between model size and performance.</p>\n</li>\n<li>\n<p><strong>Robust Error Handling</strong>: Incorporates fallback mechanisms to ensure the pipeline continues even if certain benchmarking steps fail.</p>\n</li>\n<li>\n<p><strong>Efficient Resource Utilization</strong>: Configures DeepSpeed parameters (ZeRO stages, offloading, etc.) based on hardware constraints for maximum throughput.</p>\n</li>\n</ol>\n<h3>Note</h3>\n<p>This project is now publicly available on GitHub. You can view the code repository at <a href=\"https://github.com/Angleito/qwensuicoder\">https://github.com/Angleito/qwensuicoder</a>. For any questions or collaborations, feel free to contact me at arainey555@gmail.com.</p>"
  },
  "_id": "projects/qwensuicoder.md",
  "_raw": {
    "sourceFilePath": "projects/qwensuicoder.md",
    "sourceFileName": "qwensuicoder.md",
    "sourceFileDir": "projects",
    "contentType": "markdown",
    "flattenedPath": "projects/qwensuicoder"
  },
  "type": "Project",
  "slug": "qwensuicoder",
  "url": "/projects/qwensuicoder"
}